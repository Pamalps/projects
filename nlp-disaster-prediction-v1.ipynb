{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import packages\n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-06T01:27:31.150802Z","iopub.execute_input":"2022-04-06T01:27:31.151138Z","iopub.status.idle":"2022-04-06T01:27:31.178575Z","shell.execute_reply.started":"2022-04-06T01:27:31.151060Z","shell.execute_reply":"2022-04-06T01:27:31.177930Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Import the libraries\nfrom sklearn.model_selection import train_test_split    # Splits arrays or matrices into random train and test subsets\nfrom sklearn.model_selection import KFold               # Cross-validator\nfrom sklearn.model_selection import cross_validate      # Evaluate metrics by cross-validation\nfrom sklearn.model_selection import GridSearchCV        # Search over specified parameter values for an estimator\nfrom sklearn.compose import ColumnTransformer           # Applies transformers to columns of DataFrames\nfrom sklearn.pipeline import Pipeline                   # Helps building a chain of transforms and estimators\nfrom sklearn.impute import SimpleImputer                # Imputation transformer for completing missing values\nfrom sklearn.preprocessing import OneHotEncoder         # Encode categorical features","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:31.180065Z","iopub.execute_input":"2022-04-06T01:27:31.180323Z","iopub.status.idle":"2022-04-06T01:27:32.052164Z","shell.execute_reply.started":"2022-04-06T01:27:31.180290Z","shell.execute_reply":"2022-04-06T01:27:32.051442Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Import the csv files","metadata":{}},{"cell_type":"code","source":"df_train=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", index_col='id')\ndf_test=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", index_col='id')","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:32.053485Z","iopub.execute_input":"2022-04-06T01:27:32.053722Z","iopub.status.idle":"2022-04-06T01:27:32.126806Z","shell.execute_reply.started":"2022-04-06T01:27:32.053690Z","shell.execute_reply":"2022-04-06T01:27:32.126102Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The dataset provided has been split into train and test datasets. \nThe train dataset has 7613 rows and 4 columns. The target column is part of the train dataset.\nThe test dataset has 3263 rows and 3 columns. the target column is not part of the test dataset, and needs to be predicted and submitted as part of the competition.","metadata":{}},{"cell_type":"code","source":"print(\"size of train dataset\",df_train.shape)\nprint(\"size of test dataset\",df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:32.129909Z","iopub.execute_input":"2022-04-06T01:27:32.130211Z","iopub.status.idle":"2022-04-06T01:27:32.134827Z","shell.execute_reply.started":"2022-04-06T01:27:32.130182Z","shell.execute_reply":"2022-04-06T01:27:32.134124Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"size of train dataset (7613, 4)\nsize of test dataset (3263, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test train split","metadata":{}},{"cell_type":"markdown","source":"Our model is to be developed on the provided train dataset. So we further split train dataset into train and validation datasets to create and validate different models.","metadata":{}},{"cell_type":"markdown","source":"### Split the train dataset into predictor and response variables","metadata":{}},{"cell_type":"code","source":"X = df_train.iloc[:,:-1] #the predictor columns are all columns except the target column\ny = df_train.iloc[:,-1:] #the target column is the last column ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:32.136132Z","iopub.execute_input":"2022-04-06T01:27:32.136593Z","iopub.status.idle":"2022-04-06T01:27:32.143346Z","shell.execute_reply.started":"2022-04-06T01:27:32.136558Z","shell.execute_reply":"2022-04-06T01:27:32.142556Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, \n                                                                train_size=0.8, \n                                                                test_size=0.2, \n                                                                random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:32.144614Z","iopub.execute_input":"2022-04-06T01:27:32.145302Z","iopub.status.idle":"2022-04-06T01:27:32.156222Z","shell.execute_reply.started":"2022-04-06T01:27:32.145268Z","shell.execute_reply":"2022-04-06T01:27:32.155344Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"size of X_train: predictors\",X_train_full.shape)\nprint(\"size of X_valid: predictors\",X_valid_full.shape)\nprint(\"size of y_train: response\",y_train.shape)\nprint(\"size of y_valid: response\",y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:32.157822Z","iopub.execute_input":"2022-04-06T01:27:32.158475Z","iopub.status.idle":"2022-04-06T01:27:32.165791Z","shell.execute_reply.started":"2022-04-06T01:27:32.158436Z","shell.execute_reply":"2022-04-06T01:27:32.164896Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"size of X_train: predictors (6090, 3)\nsize of X_valid: predictors (1523, 3)\nsize of y_train: response (6090, 1)\nsize of y_valid: response (1523, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"Exploratory analysis on our new training dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:32:27.741118Z","iopub.execute_input":"2022-04-03T21:32:27.741747Z","iopub.status.idle":"2022-04-03T21:32:27.748495Z","shell.execute_reply.started":"2022-04-03T21:32:27.741685Z","shell.execute_reply":"2022-04-03T21:32:27.74706Z"}}},{"cell_type":"code","source":"X_train_full.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:32.167341Z","iopub.execute_input":"2022-04-06T01:27:32.167644Z","iopub.status.idle":"2022-04-06T01:27:32.185131Z","shell.execute_reply.started":"2022-04-06T01:27:32.167604Z","shell.execute_reply":"2022-04-06T01:27:32.184372Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 6090 entries, 1999 to 3924\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   keyword   6044 non-null   object\n 1   location  4075 non-null   object\n 2   text      6090 non-null   object\ndtypes: object(3)\nmemory usage: 190.3+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We see that location and keywords have some nulls.","metadata":{}},{"cell_type":"markdown","source":"## check the balance of the dataset","metadata":{}},{"cell_type":"code","source":"y_train.groupby(['target']).size()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:38.814931Z","iopub.execute_input":"2022-04-06T01:27:38.815535Z","iopub.status.idle":"2022-04-06T01:27:38.830130Z","shell.execute_reply.started":"2022-04-06T01:27:38.815492Z","shell.execute_reply":"2022-04-06T01:27:38.829286Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"target\n0    3456\n1    2634\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"So we see that its a well balanced dataset.","metadata":{}},{"cell_type":"markdown","source":"## check examples of disaster and non disaster tweets","metadata":{}},{"cell_type":"code","source":"df_train[df_train['target']==0].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:47.371053Z","iopub.execute_input":"2022-04-06T01:27:47.371583Z","iopub.status.idle":"2022-04-06T01:27:47.394622Z","shell.execute_reply.started":"2022-04-06T01:27:47.371541Z","shell.execute_reply":"2022-04-06T01:27:47.393970Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   keyword location                          text  target\nid                                                       \n23     NaN      NaN                What's up man?       0\n24     NaN      NaN                 I love fruits       0\n25     NaN      NaN              Summer is lovely       0\n26     NaN      NaN             My car is so fast       0\n28     NaN      NaN  What a goooooooaaaaaal!!!!!!       0\n31     NaN      NaN        this is ridiculous....       0\n32     NaN      NaN             London is cool ;)       0\n33     NaN      NaN                   Love skiing       0\n34     NaN      NaN         What a wonderful day!       0\n36     NaN      NaN                      LOOOOOOL       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What's up man?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I love fruits</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Summer is lovely</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>My car is so fast</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What a goooooooaaaaaal!!!!!!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>this is ridiculous....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>London is cool ;)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Love skiing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What a wonderful day!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LOOOOOOL</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train[df_train['target']==1].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:27:53.319020Z","iopub.execute_input":"2022-04-06T01:27:53.319576Z","iopub.status.idle":"2022-04-06T01:27:53.332492Z","shell.execute_reply.started":"2022-04-06T01:27:53.319537Z","shell.execute_reply":"2022-04-06T01:27:53.331828Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   keyword location                                               text  target\nid                                                                            \n1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1\n8      NaN      NaN  #RockyFire Update => California Hwy. 20 closed...       1\n10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...       1\n13     NaN      NaN  I'm on top of the hill and I can see a fire in...       1\n14     NaN      NaN  There's an emergency evacuation happening now ...       1\n15     NaN      NaN  I'm afraid that the tornado is coming to our a...       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"For a person reading the tweets, its quite easy to understand which is a disaster and which one is not.","metadata":{}},{"cell_type":"markdown","source":"# LSTM model","metadata":{}},{"cell_type":"code","source":"#import tf\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:04:48.001401Z","iopub.execute_input":"2022-04-03T22:04:48.001984Z","iopub.status.idle":"2022-04-03T22:04:55.464673Z","shell.execute_reply.started":"2022-04-03T22:04:48.001937Z","shell.execute_reply":"2022-04-03T22:04:55.463634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We train our model on only the text column.","metadata":{}},{"cell_type":"code","source":"training_sentences = X_train_full['text']\nvalidation_sentences = X_valid_full['text']\ntraining_labels = y_train\nvalidation_labels = y_valid","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:10:37.414955Z","iopub.execute_input":"2022-04-03T22:10:37.415287Z","iopub.status.idle":"2022-04-03T22:10:37.420776Z","shell.execute_reply.started":"2022-04-03T22:10:37.415252Z","shell.execute_reply":"2022-04-03T22:10:37.419812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_labels_final = np.array(training_labels)\nvalidation_labels_final = np.array(validation_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:13:40.765474Z","iopub.execute_input":"2022-04-03T22:13:40.765921Z","iopub.status.idle":"2022-04-03T22:13:40.771624Z","shell.execute_reply.started":"2022-04-03T22:13:40.76589Z","shell.execute_reply":"2022-04-03T22:13:40.770366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=100000, oov_token='<OOV>')\ntokenizer.fit_on_texts(training_sentences)\n\ntokenizer = Tokenizer(num_words=100000, oov_token='<OOV>')\ntokenizer.fit_on_texts(validation_sentences)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:12:11.862876Z","iopub.execute_input":"2022-04-03T22:12:11.863496Z","iopub.status.idle":"2022-04-03T22:12:12.103764Z","shell.execute_reply.started":"2022-04-03T22:12:11.863452Z","shell.execute_reply":"2022-04-03T22:12:12.102619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_sequences = tokenizer.texts_to_sequences(training_sentences)\npad_training = pad_sequences(training_sequences, maxlen=25, padding='post', truncating='post')\n\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\npad_validation = pad_sequences(validation_sequences, maxlen=25, padding='post', truncating='post')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:12:21.890485Z","iopub.execute_input":"2022-04-03T22:12:21.890832Z","iopub.status.idle":"2022-04-03T22:12:22.102455Z","shell.execute_reply.started":"2022-04-03T22:12:21.890797Z","shell.execute_reply":"2022-04-03T22:12:22.101412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the LSTM model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(100000, 16, input_length=20),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:12:30.298426Z","iopub.execute_input":"2022-04-03T22:12:30.298772Z","iopub.status.idle":"2022-04-03T22:12:31.602374Z","shell.execute_reply.started":"2022-04-03T22:12:30.298728Z","shell.execute_reply":"2022-04-03T22:12:31.601545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:12:45.612427Z","iopub.execute_input":"2022-04-03T22:12:45.612759Z","iopub.status.idle":"2022-04-03T22:12:45.631625Z","shell.execute_reply.started":"2022-04-03T22:12:45.612727Z","shell.execute_reply":"2022-04-03T22:12:45.630237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(pad_training, training_labels_final, epochs=15, validation_data=(pad_validation, validation_labels_final))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:13:46.90184Z","iopub.execute_input":"2022-04-03T22:13:46.902358Z","iopub.status.idle":"2022-04-03T22:18:51.264953Z","shell.execute_reply.started":"2022-04-03T22:13:46.902316Z","shell.execute_reply":"2022-04-03T22:18:51.264186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the accuracy\nimport matplotlib.pyplot as plt\ndef plot_graph(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:23:15.19519Z","iopub.execute_input":"2022-04-03T22:23:15.195544Z","iopub.status.idle":"2022-04-03T22:23:15.201705Z","shell.execute_reply.started":"2022-04-03T22:23:15.195509Z","shell.execute_reply":"2022-04-03T22:23:15.200555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graph(history, 'accuracy')\nplot_graph(history, 'loss')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:23:15.851521Z","iopub.execute_input":"2022-04-03T22:23:15.852756Z","iopub.status.idle":"2022-04-03T22:23:16.276704Z","shell.execute_reply.started":"2022-04-03T22:23:15.852697Z","shell.execute_reply":"2022-04-03T22:23:16.275788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict","metadata":{}},{"cell_type":"code","source":"\ntest_sequences = tokenizer.texts_to_sequences(df_test.text)\npad_test = pad_sequences(test_sequences,maxlen=25, padding='post', truncating='post')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:24:29.220209Z","iopub.execute_input":"2022-04-03T22:24:29.22057Z","iopub.status.idle":"2022-04-03T22:24:29.317538Z","shell.execute_reply.started":"2022-04-03T22:24:29.220528Z","shell.execute_reply":"2022-04-03T22:24:29.316505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the submission files\nsubmission=pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\", index_col='id')\nprediction = model.predict(pad_test)\nsubmission['target'] = (prediction>0.5).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:25:27.53408Z","iopub.execute_input":"2022-04-03T22:25:27.534476Z","iopub.status.idle":"2022-04-03T22:25:29.810323Z","shell.execute_reply.started":"2022-04-03T22:25:27.534435Z","shell.execute_reply":"2022-04-03T22:25:29.809021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:36:24.401685Z","iopub.execute_input":"2022-04-03T22:36:24.403116Z","iopub.status.idle":"2022-04-03T22:36:24.418606Z","shell.execute_reply.started":"2022-04-03T22:36:24.403066Z","shell.execute_reply":"2022-04-03T22:36:24.417471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output the csv file\n# submission.to_csv('submission1_lstm.csv', index=id, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T01:31:14.888522Z","iopub.execute_input":"2022-04-06T01:31:14.888793Z","iopub.status.idle":"2022-04-06T01:31:14.892605Z","shell.execute_reply.started":"2022-04-06T01:31:14.888765Z","shell.execute_reply":"2022-04-06T01:31:14.891402Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf_train=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", index_col='id')\ndf_test=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", index_col='id')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:34:54.257434Z","iopub.execute_input":"2022-04-04T02:34:54.257812Z","iopub.status.idle":"2022-04-04T02:34:54.370923Z","shell.execute_reply.started":"2022-04-04T02:34:54.257694Z","shell.execute_reply":"2022-04-04T02:34:54.369929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clean the dataset","metadata":{}},{"cell_type":"code","source":"import re\ndef clean_text(text):\n    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    text = text.lower()  \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:34:55.053553Z","iopub.execute_input":"2022-04-04T02:34:55.05418Z","iopub.status.idle":"2022-04-04T02:34:55.183064Z","shell.execute_reply.started":"2022-04-04T02:34:55.054143Z","shell.execute_reply":"2022-04-04T02:34:55.182042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['clean_text'] = df_train.text.apply(clean_text)\ndf_test['clean_text'] = df_test.text.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:34:55.805058Z","iopub.execute_input":"2022-04-04T02:34:55.805697Z","iopub.status.idle":"2022-04-04T02:34:55.869575Z","shell.execute_reply.started":"2022-04-04T02:34:55.80566Z","shell.execute_reply":"2022-04-04T02:34:55.868604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test=df_test[['keyword','location','text','clean_text']]","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:34:56.404696Z","iopub.execute_input":"2022-04-04T02:34:56.405427Z","iopub.status.idle":"2022-04-04T02:34:56.415417Z","shell.execute_reply.started":"2022-04-04T02:34:56.405377Z","shell.execute_reply":"2022-04-04T02:34:56.414415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['target']=0","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:34:57.019051Z","iopub.execute_input":"2022-04-04T02:34:57.019719Z","iopub.status.idle":"2022-04-04T02:34:57.026007Z","shell.execute_reply.started":"2022-04-04T02:34:57.019682Z","shell.execute_reply":"2022-04-04T02:34:57.024931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:34:57.639163Z","iopub.execute_input":"2022-04-04T02:34:57.639546Z","iopub.status.idle":"2022-04-04T02:34:59.315671Z","shell.execute_reply.started":"2022-04-04T02:34:57.639501Z","shell.execute_reply":"2022-04-04T02:34:59.314694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train=df_train[['keyword','location','text','clean_text','target']]","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:04.466258Z","iopub.execute_input":"2022-04-04T02:35:04.467339Z","iopub.status.idle":"2022-04-04T02:35:04.477677Z","shell.execute_reply.started":"2022-04-04T02:35:04.467283Z","shell.execute_reply":"2022-04-04T02:35:04.476642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the lists of sentences and their labels.\nsentences = df_train.clean_text.values\nlabels = df_train.target.values","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:04.480251Z","iopub.execute_input":"2022-04-04T02:35:04.480898Z","iopub.status.idle":"2022-04-04T02:35:04.491984Z","shell.execute_reply.started":"2022-04-04T02:35:04.48085Z","shell.execute_reply":"2022-04-04T02:35:04.490924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Bert Base Uncased","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer.\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:06.117757Z","iopub.execute_input":"2022-04-04T02:35:06.1184Z","iopub.status.idle":"2022-04-04T02:35:12.385877Z","shell.execute_reply.started":"2022-04-04T02:35:06.118366Z","shell.execute_reply":"2022-04-04T02:35:12.384885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = []\n\nfor sent in sentences:\n    encoded_sent = tokenizer.encode(\n                        sent\n                   )\n    input_ids.append(encoded_sent)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:12.388213Z","iopub.execute_input":"2022-04-04T02:35:12.388549Z","iopub.status.idle":"2022-04-04T02:35:18.880525Z","shell.execute_reply.started":"2022-04-04T02:35:12.388505Z","shell.execute_reply":"2022-04-04T02:35:18.879564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Max sentence length: ', max([len(sen) for sen in input_ids]))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:20.75462Z","iopub.execute_input":"2022-04-04T02:35:20.754918Z","iopub.status.idle":"2022-04-04T02:35:20.764991Z","shell.execute_reply.started":"2022-04-04T02:35:20.754886Z","shell.execute_reply":"2022-04-04T02:35:20.763904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nMAX_LEN = 64\n#Padding the input to the max length that is 64\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n                          value=0, truncating=\"post\", padding=\"post\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:21.701529Z","iopub.execute_input":"2022-04-04T02:35:21.702123Z","iopub.status.idle":"2022-04-04T02:35:22.434917Z","shell.execute_reply.started":"2022-04-04T02:35:21.702087Z","shell.execute_reply":"2022-04-04T02:35:22.433888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the attention masks\nattention_masks = []\n\nfor sent in input_ids:\n    att_mask = [int(token_id > 0) for token_id in sent]\n    attention_masks.append(att_mask)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:23.167435Z","iopub.execute_input":"2022-04-04T02:35:23.168085Z","iopub.status.idle":"2022-04-04T02:35:23.57899Z","shell.execute_reply.started":"2022-04-04T02:35:23.168048Z","shell.execute_reply":"2022-04-04T02:35:23.578032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=2018, test_size=0.1)\n\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n                                             random_state=2018, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:26.000389Z","iopub.execute_input":"2022-04-04T02:35:26.001351Z","iopub.status.idle":"2022-04-04T02:35:26.708224Z","shell.execute_reply.started":"2022-04-04T02:35:26.001316Z","shell.execute_reply":"2022-04-04T02:35:26.707253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting the input data to the tensor , which can be fed to the model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\n\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\n\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:27.47538Z","iopub.execute_input":"2022-04-04T02:35:27.476054Z","iopub.status.idle":"2022-04-04T02:35:27.56163Z","shell.execute_reply.started":"2022-04-04T02:35:27.476017Z","shell.execute_reply":"2022-04-04T02:35:27.560625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#Creating the DataLoader which will help us to load data into the GPU/CPU\nbatch_size = 32\n\n# Create the DataLoader for our training set.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:28.349401Z","iopub.execute_input":"2022-04-04T02:35:28.349828Z","iopub.status.idle":"2022-04-04T02:35:28.357751Z","shell.execute_reply.started":"2022-04-04T02:35:28.349791Z","shell.execute_reply":"2022-04-04T02:35:28.356252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load pretrained BERT model","metadata":{}},{"cell_type":"code","source":"#Loading the pre-trained BERT model from huggingface library\n\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\n\n# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", \n    num_labels = 2,   \n    output_attentions = False, \n    output_hidden_states = False, )\n\n# Teeling the model to run on GPU\nmodel.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:29.408573Z","iopub.execute_input":"2022-04-04T02:35:29.408871Z","iopub.status.idle":"2022-04-04T02:35:54.250934Z","shell.execute_reply.started":"2022-04-04T02:35:29.408838Z","shell.execute_reply":"2022-04-04T02:35:54.249941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 2e-5, \n                  eps = 1e-8 \n                )","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:35:54.252781Z","iopub.execute_input":"2022-04-04T02:35:54.253804Z","iopub.status.idle":"2022-04-04T02:35:54.26762Z","shell.execute_reply.started":"2022-04-04T02:35:54.253754Z","shell.execute_reply":"2022-04-04T02:35:54.26644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\nepochs = 4\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)\nscheduler","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:36:01.183348Z","iopub.execute_input":"2022-04-04T02:36:01.18412Z","iopub.status.idle":"2022-04-04T02:36:01.19298Z","shell.execute_reply.started":"2022-04-04T02:36:01.184083Z","shell.execute_reply":"2022-04-04T02:36:01.191567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:36:01.946323Z","iopub.execute_input":"2022-04-04T02:36:01.94666Z","iopub.status.idle":"2022-04-04T02:36:01.952666Z","shell.execute_reply.started":"2022-04-04T02:36:01.946626Z","shell.execute_reply":"2022-04-04T02:36:01.951512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:36:02.972623Z","iopub.execute_input":"2022-04-04T02:36:02.973225Z","iopub.status.idle":"2022-04-04T02:36:02.981034Z","shell.execute_reply.started":"2022-04-04T02:36:02.973156Z","shell.execute_reply":"2022-04-04T02:36:02.979993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model training","metadata":{}},{"cell_type":"code","source":"import random\n\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nloss_values = []\n\nfor epoch_i in range(0, epochs):\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()        \n\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n\n        loss = outputs[0]\n\n        total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss / len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    model.eval()\n\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        with torch.no_grad():        \n\n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n\n        # Track the number of batches\n        nb_eval_steps += 1\n\n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n\nprint(\"\")\nprint(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:36:03.79434Z","iopub.execute_input":"2022-04-04T02:36:03.795083Z","iopub.status.idle":"2022-04-04T02:39:21.681795Z","shell.execute_reply.started":"2022-04-04T02:36:03.795032Z","shell.execute_reply":"2022-04-04T02:39:21.680809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(loss_values) ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:39:36.970908Z","iopub.execute_input":"2022-04-04T02:39:36.971216Z","iopub.status.idle":"2022-04-04T02:39:36.977649Z","shell.execute_reply.started":"2022-04-04T02:39:36.971162Z","shell.execute_reply":"2022-04-04T02:39:36.976333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = df_test.clean_text.values\nlabels = df_test.target.values","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:39:39.790387Z","iopub.execute_input":"2022-04-04T02:39:39.790898Z","iopub.status.idle":"2022-04-04T02:39:39.796775Z","shell.execute_reply.started":"2022-04-04T02:39:39.790862Z","shell.execute_reply":"2022-04-04T02:39:39.795763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize all of the sentences and map the tokens to thier word IDs.\ninput_ids = []\n\n# For every sentence...\nfor sent in sentences:\n    encoded_sent = tokenizer.encode(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                   )\n    \n    input_ids.append(encoded_sent)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:39:40.622292Z","iopub.execute_input":"2022-04-04T02:39:40.624402Z","iopub.status.idle":"2022-04-04T02:39:43.609357Z","shell.execute_reply.started":"2022-04-04T02:39:40.624348Z","shell.execute_reply":"2022-04-04T02:39:43.608349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask) \n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:40:42.04246Z","iopub.execute_input":"2022-04-04T02:40:42.042754Z","iopub.status.idle":"2022-04-04T02:40:42.258737Z","shell.execute_reply.started":"2022-04-04T02:40:42.042721Z","shell.execute_reply":"2022-04-04T02:40:42.257755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to tensors.\nprediction_inputs = torch.tensor(input_ids)\nprediction_masks = torch.tensor(attention_masks)\nprediction_labels = torch.tensor(labels)\n\n# Set the batch size.  \nbatch_size = 32  ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:40:46.960786Z","iopub.execute_input":"2022-04-04T02:40:46.961417Z","iopub.status.idle":"2022-04-04T02:40:46.987525Z","shell.execute_reply.started":"2022-04-04T02:40:46.961362Z","shell.execute_reply":"2022-04-04T02:40:46.986543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the DataLoader.\nprediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:40:47.850431Z","iopub.execute_input":"2022-04-04T02:40:47.850729Z","iopub.status.idle":"2022-04-04T02:40:47.856872Z","shell.execute_reply.started":"2022-04-04T02:40:47.850697Z","shell.execute_reply":"2022-04-04T02:40:47.855361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n\nmodel.eval()\n\npredictions , true_labels = [], []","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:40:51.151616Z","iopub.execute_input":"2022-04-04T02:40:51.15191Z","iopub.status.idle":"2022-04-04T02:40:51.159407Z","shell.execute_reply.started":"2022-04-04T02:40:51.151879Z","shell.execute_reply":"2022-04-04T02:40:51.15811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict \nfor batch in prediction_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n\n  logits = outputs[0]\n\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  label_ids = b_labels.to('cpu').numpy()\n  \n  # Store predictions and true labels\n  predictions.append(logits)\n  true_labels.append(label_ids)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:40:52.044414Z","iopub.execute_input":"2022-04-04T02:40:52.044714Z","iopub.status.idle":"2022-04-04T02:40:58.412992Z","shell.execute_reply.started":"2022-04-04T02:40:52.044682Z","shell.execute_reply":"2022-04-04T02:40:58.411968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:40:58.418343Z","iopub.execute_input":"2022-04-04T02:40:58.418631Z","iopub.status.idle":"2022-04-04T02:40:58.430634Z","shell.execute_reply.started":"2022-04-04T02:40:58.4186Z","shell.execute_reply":"2022-04-04T02:40:58.427241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(flat_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:41:00.687093Z","iopub.execute_input":"2022-04-04T02:41:00.687482Z","iopub.status.idle":"2022-04-04T02:41:00.704957Z","shell.execute_reply.started":"2022-04-04T02:41:00.687448Z","shell.execute_reply":"2022-04-04T02:41:00.703665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=pd.DataFrame(flat_predictions,index=df_test.index)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:48:24.308339Z","iopub.execute_input":"2022-04-04T02:48:24.308705Z","iopub.status.idle":"2022-04-04T02:48:24.314586Z","shell.execute_reply.started":"2022-04-04T02:48:24.308655Z","shell.execute_reply":"2022-04-04T02:48:24.312632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=predictions.rename(columns={0: \"target\"})","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:48:31.089992Z","iopub.execute_input":"2022-04-04T02:48:31.090459Z","iopub.status.idle":"2022-04-04T02:48:31.09626Z","shell.execute_reply.started":"2022-04-04T02:48:31.090423Z","shell.execute_reply":"2022-04-04T02:48:31.095141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write ouput predictions\npredictions.to_csv('submission2_bert.csv', index=id, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T02:48:37.299021Z","iopub.execute_input":"2022-04-04T02:48:37.3Z","iopub.status.idle":"2022-04-04T02:48:37.314845Z","shell.execute_reply.started":"2022-04-04T02:48:37.299951Z","shell.execute_reply":"2022-04-04T02:48:37.313702Z"},"trusted":true},"execution_count":null,"outputs":[]}]}